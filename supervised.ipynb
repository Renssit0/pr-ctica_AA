{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression, SGDClassifier\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import F1Score\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow import multiply\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from itertools import product\n",
    "from tensorflow.random import set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\david\\documents\\uni\\aprendizaje supervisado\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached protobuf-5.28.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\david\\documents\\uni\\aprendizaje supervisado\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\david\\documents\\uni\\aprendizaje supervisado\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\david\\documents\\uni\\aprendizaje supervisado\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached grpcio-1.67.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached numpy-2.0.2-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached h5py-3.12.1-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached wheel-0.45.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached optree-0.13.0-cp311-cp311-win_amd64.whl.metadata (48 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\david\\documents\\uni\\aprendizaje supervisado\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl (7.5 kB)\n",
      "Using cached tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl (390.2 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.67.1-cp311-cp311-win_amd64.whl (4.4 MB)\n",
      "Using cached h5py-3.12.1-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "Using cached keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl (126 kB)\n",
      "Using cached numpy-2.0.2-cp311-cp311-win_amd64.whl (15.9 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached protobuf-5.28.3-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl (101 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached wheel-0.45.0-py3-none-any.whl (72 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.13.0-cp311-cp311-win_amd64.whl (283 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, absl-py, werkzeug, requests, ml-dtypes, markdown-it-py, h5py, astunparse, tensorboard, rich, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.1.0 astunparse-1.6.3 certifi-2024.8.30 charset-normalizer-3.4.0 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.67.1 h5py-3.12.1 idna-3.10 keras-3.6.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 opt-einsum-3.4.0 optree-0.13.0 protobuf-5.28.3 requests-2.32.3 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.5.0 urllib3-2.2.3 werkzeug-3.1.3 wheel-0.45.0 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos una búsqueda aleatorio de los mejores hiperparámetros, esto es debido a que una búsqueda más exhaustiva para un dataset tan grande es demasiado ineficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regresionLogistica(X, y, n, v_cruzada, seed):\n",
    "    log_reg = LogisticRegression(max_iter=500)\n",
    "    parametros_log_reg = {'C': np.logspace(-4, 4, 20), 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
    "    busqueda_log_reg = RandomizedSearchCV(estimator=log_reg, param_distributions=parametros_log_reg, n_iter=n, \\\n",
    "                                      scoring='f1_weighted', cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_log_reg.fit(X, y)\n",
    "    parametros = busqueda_log_reg.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_log_reg.best_score_}')\n",
    "\n",
    "    modelo = LogisticRegression(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def SGDC(X, y, n, v_cruzada, seed):\n",
    "    sgd = SGDClassifier()\n",
    "    parametros_sgdc = {'loss': ['hinge', 'squared_error', 'log_loss'], 'alpha': np.logspace(-5, 0, 10),\\\n",
    "                       'max_iter': [100, 200], 'tol': [1e-3, 1e-4]}\n",
    "    busqueda_sgdc = RandomizedSearchCV(estimator=sgd, param_distributions=parametros_sgdc, n_iter=n,\\\n",
    "                                       scoring='f1_weighted', cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_sgdc.fit(X, y)\n",
    "    parametros = busqueda_sgdc.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_sgdc.best_score_}')\n",
    "\n",
    "    modelo = SGDClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n",
    "def bosqueAleatorio(X, y, n, v_cruzada, seed):\n",
    "    rf = RandomForestClassifier()\n",
    "    parametros_rf = {'n_estimators': np.arange(100, 200, 20), 'max_features': ['sqrt', 'log2']+list(range(1, 10, 3)), \\\n",
    "                     'max_depth': list(np.arange(5, 20, 5)), 'min_samples_split': np.arange(5, 20, 5)}\n",
    "    busqueda_rf = RandomizedSearchCV(estimator=rf, param_distributions=parametros_rf, n_iter=n, scoring='f1_weighted',\\\n",
    "                                   cv = v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_rf.fit(X, y)\n",
    "    parametros = busqueda_rf.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_rf.best_score_}')\n",
    "\n",
    "    modelo = RandomForestClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def arbolDecision(X, y, n, v_cruzada, seed):\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    parametros_dtc = {'max_depth': list(np.arange(20, 50, 2)), 'min_samples_split': np.arange(10, 20, 2), 'criterion': ['gini', 'entropy']}\n",
    "    busqueda_dtc = RandomizedSearchCV(estimator=dtc, param_distributions=parametros_dtc, n_iter=n, scoring='f1_weighted',\\\n",
    "                                      cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_dtc.fit(X, y)\n",
    "    parametros = busqueda_dtc.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_dtc.best_score_}')\n",
    "\n",
    "    modelo = DecisionTreeClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n",
    "def redNeuronal(X, y, n, v_cruzada, seed):\n",
    "    set_seed(seed)\n",
    "    def design_model(n_features, lr):\n",
    "        input_ = Input(shape=(n_features,))\n",
    "        layer1 = Dropout(0.3)(input_)\n",
    "        layer2 = Dense(128, kernel_regularizer=L2)(layer1)\n",
    "        layer3 = Dropout(0.3)(layer2)\n",
    "        layer4 = Dense(128, kernel_regularizer=L2)(layer3)\n",
    "        output = Dense(1, activation=\"sigmoid\")(layer4)\n",
    "\n",
    "        # model\n",
    "        model = Model(inputs=input_, outputs=output)\n",
    "        model.compile(loss=BinaryCrossentropy, metrics=[F1Score], optimizer = Adam(learning_rate=lr))\n",
    "        return model\n",
    "    \n",
    "    n_features = X.shape[-1]\n",
    "    stop = EarlyStopping(monitor='val_loss', mode='min', patience=50, restore_best_weights=True)\n",
    "    parameter_grid = product((0.1, 0.01, 0.001), (16, 64, 256))\n",
    "\n",
    "    results = list()\n",
    "    for comb in parameter_grid:\n",
    "        model = design_model(n_features, lr=comb[0])\n",
    "        history = model.fit(X,y,epochs=500, batch_size=comb[1], validation_split=0.2,\n",
    "                        callbacks = [stop], verbose=0)\n",
    "        results.append((comb[0], comb[1], history.history['f1_score'][-1]))\n",
    "    \n",
    "    best_param = max(sorted(results, key = lambda x: x[2]))\n",
    "    print(f'Los mejores parámetros son: learning_rate: {best_param[0]}, num_batches: {best_param[1]} con una puntuación de {best_param[2]}')\n",
    "\n",
    "    model = design_model(n_features, lr=best_param[0])\n",
    "    scores = np.empty(v_cruzada)\n",
    "    n_samples = X.shape[0]\n",
    "    jump = int(n_samples/v_cruzada)\n",
    "    for i in range(v_cruzada):\n",
    "        val_idx = range(i*jump,(i+1)*jump)\n",
    "        model.fit(X.drop(X.iloc[val_idx].index), y.drop(y.iloc[val_idx].index),epochs=500, batch_size=comb[1], validation_data=(X.iloc[val_idx],y.iloc[val_idx]),\n",
    "                callbacks = [stop], verbose=0)\n",
    "        scores[i] = stop.best\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def KNN(X, y, n, v_cruzada, seed):\n",
    "    knn = KNeighborsClassifier()\n",
    "    parametros_knn = {'n_neighbors': np.arange(1, 15, 3), 'weights': ['uniform', 'distance'],\\\n",
    "                      'metric': ['euclidean', 'manhattan']}\n",
    "    busqueda_knn = RandomizedSearchCV(estimator=knn, param_distributions=parametros_knn, n_iter=n,\\\n",
    "                                    scoring='f1_weighted', cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_knn.fit(X, y)\n",
    "    parametros = busqueda_knn.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_knn.best_score_}')\n",
    "\n",
    "    modelo = KNeighborsClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if TensorFlow can detect the GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Available GPUs: {gpus}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_cruzada = 5\n",
    "seed = 34\n",
    "n = 30\n",
    "df = pd.read_csv('df.csv', index_col=0).sample(n=50000, random_state=seed)\n",
    "df_over = pd.read_csv('df_over.csv', index_col=0).sample(n=50000, random_state=seed)\n",
    "\n",
    "def read_pickle(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        columns = pickle.load(file)\n",
    "    return columns\n",
    "\n",
    "def hyperparameter_tuning_general(path):\n",
    "    df_rdx = df[read_pickle(path)]\n",
    "    X, y = df_rdx.drop('label', axis=1), df_rdx['label']\n",
    "    print(regresionLogistica(X, y, n, v_cruzada, seed))\n",
    "    print(SGDC(X, y, n, v_cruzada, seed))\n",
    "    print(bosqueAleatorio(X, y, n, v_cruzada, seed))\n",
    "    print(arbolDecision(X, y, n, v_cruzada, seed))\n",
    "    print(redNeuronal(X, y, n, v_cruzada, seed))\n",
    "    print(KNN(X, y, n, v_cruzada, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: learning_rate: 0.1, num_batches: 256 con una puntuación de 0.4108065068721771\n",
      "0.3101118505001068\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning_general('df1_columns.pkl')\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(1438.44988828766)} con una puntuación de 0.8836523703898143\n",
    "# 0.8836523703898143\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(1e-05)} con una puntuación de 0.8830713771651768\n",
    "# 0.8705891820233645\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(100), 'min_samples_split': np.int64(15), 'max_features': 'log2', 'max_depth': np.int64(15)} con una puntuación de 0.8822279091007633\n",
    "# 0.8802700901684221\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(18), 'max_depth': np.int64(22), 'criterion': 'gini'} con una puntuación de 0.8297647106742028\n",
    "# 0.8292204390533506\n",
    "\n",
    "# arreglar tuning, ventaja a quien converge mas rapido\n",
    "# Los mejores parámetros son: learning_rate: 0.1, num_batches: 256 con una puntuación de 0.4108065068721771\n",
    "# 0.3101118505001068\n",
    "\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# Los mejores parámetros son: {'weights': 'uniform', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8381402494954733\n",
    "# 0.8381402494954733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': np.float64(11.288378916846883)} con una puntuación de 0.8790116083084459\n",
      "0.8789239112218639\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.001, 'max_iter': 200, 'loss': 'log_loss', 'alpha': np.float64(1e-05)} con una puntuación de 0.8763173410753795\n",
      "0.870363909928772\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(10), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.8798856772816105\n",
      "0.8797234595037231\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8281086733314857\n",
      "0.8277098130939763\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8376863481234462\n",
      "0.8376863481234462\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning_general('df1_over_columns.pkl')\n",
    "\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': np.float64(11.288378916846883)} con una puntuación de 0.8790116083084459\n",
    "# 0.8789239112218639\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.001, 'max_iter': 200, 'loss': 'log_loss', 'alpha': np.float64(1e-05)} con una puntuación de 0.8763173410753795\n",
    "# 0.870363909928772\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(10), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.8798856772816105\n",
    "# 0.8797234595037231\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8281086733314857\n",
    "# 0.8277098130939763\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8376863481234462\n",
    "# 0.8376863481234462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': np.float64(1.623776739188721)} con una puntuación de 0.8686068480450551\n",
      "0.8685884377243067\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(0.0001291549665014884)} con una puntuación de 0.869123169398431\n",
      "0.8680290383424178\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(5), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.870872257158875\n",
      "0.8693060540725103\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8238308603518425\n",
      "0.8229223637389049\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: {'weights': 'uniform', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8359992668647557\n",
      "0.8359992668647557\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning_general('df21_columns.pkl')\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': np.float64(1.623776739188721)} con una puntuación de 0.8686068480450551\n",
    "# 0.8685884377243067\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(0.0001291549665014884)} con una puntuación de 0.869123169398431\n",
    "# 0.8680290383424178\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(5), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.870872257158875\n",
    "# 0.8693060540725103\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8238308603518425\n",
    "# 0.8229223637389049\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# Los mejores parámetros son: {'weights': 'uniform', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8359992668647557\n",
    "# 0.8359992668647557"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': np.float64(3792.690190732246)} con una puntuación de 0.8690637259804713\n",
      "0.8689902079249718\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.001, 'max_iter': 200, 'loss': 'hinge', 'alpha': np.float64(0.0016681005372000592)} con una puntuación de 0.8695568774417272\n",
      "0.8666602968564856\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(10), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.8706375912843614\n",
      "0.8699943036851534\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8232128288147627\n",
      "0.8231848480889429\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8367686275629154\n",
      "0.8367686275629154\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning_general('df21_over_columns.pkl')\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': np.float64(3792.690190732246)} con una puntuación de 0.8690637259804713\n",
    "# 0.8689902079249718\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.001, 'max_iter': 200, 'loss': 'hinge', 'alpha': np.float64(0.0016681005372000592)} con una puntuación de 0.8695568774417272\n",
    "# 0.8666602968564856\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(10), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.8706375912843614\n",
    "# 0.8699943036851534\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8232128288147627\n",
    "# 0.8231848480889429\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8367686275629154\n",
    "# 0.8367686275629154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(10000.0)} con una puntuación de 0.8817349147911596\n",
      "0.8817349147911596\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(0.0001291549665014884)} con una puntuación de 0.8797487306986795\n",
      "0.877929648312875\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(10), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.8811952711092832\n",
      "0.8803406313736026\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8308660422307245\n",
      "0.8308985042919395\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8363056535870994\n",
      "0.8363056535870994\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning_general('df22_columns.pkl')\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(10000.0)} con una puntuación de 0.8817349147911596\n",
    "# 0.8817349147911596\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(0.0001291549665014884)} con una puntuación de 0.8797487306986795\n",
    "# 0.877929648312875\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(10), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.8811952711092832\n",
    "# 0.8803406313736026\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8308660422307245\n",
    "# 0.8308985042919395\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8363056535870994\n",
    "# 0.8363056535870994\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(3792.690190732246)} con una puntuación de 0.8819852792810661\n",
      "0.8819852792810661\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(0.0001291549665014884)} con una puntuación de 0.8802596768085641\n",
      "0.8802367991149194\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(5), 'max_features': 'log2', 'max_depth': np.int64(15)} con una puntuación de 0.8807910837927398\n",
      "0.8799058676583137\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8313389385364051\n",
      "0.8320685712582645\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8374412069902707\n",
      "0.8374412069902707\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning_general('df22_over_columns.pkl')\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(3792.690190732246)} con una puntuación de 0.8819852792810661\n",
    "# 0.8819852792810661\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(0.0001291549665014884)} con una puntuación de 0.8802596768085641\n",
    "# 0.8802367991149194\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(5), 'max_features': 'log2', 'max_depth': np.int64(15)} con una puntuación de 0.8807910837927398\n",
    "# 0.8799058676583137\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8313389385364051\n",
    "# 0.8320685712582645\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
    "#   warnings.warn(\n",
    "# Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8374412069902707\n",
    "# 0.8374412069902707\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning_specific(df_columns, name, model):\n",
    "    df_rdx = df[df_columns[name]]\n",
    "    X, y = df_rdx.drop('label', axis=1), df_rdx['label'] \n",
    "    print(name, model(X, y, n, v_cruzada, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(29.763514416313132)} con una puntuación de 0.8307701271904486\n",
      "LogisticRegression 0.8307701271904486\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 200, 'loss': 'hinge', 'alpha': np.float64(3.5938136638046256e-05)} con una puntuación de 0.8355134628645823\n",
      "SGDClassifier 0.8363256270804058\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(15), 'max_features': 'sqrt', 'max_depth': np.int64(5)} con una puntuación de 0.8407902015725431\n",
      "RandomForestClassifier 0.8403833332635926\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'entropy'} con una puntuación de 0.8166143581770171\n",
      "DecisionTreeClassifier 0.8166258721117126\n"
     ]
    }
   ],
   "source": [
    "df3_columns = read_pickle('df3_columns.pkl')\n",
    "\n",
    "hyperparameter_tuning_specific(df3_columns, 'LogisticRegression', regresionLogistica)\n",
    "hyperparameter_tuning_specific(df3_columns, 'SGDClassifier', SGDC)\n",
    "hyperparameter_tuning_specific(df3_columns, 'RandomForestClassifier', bosqueAleatorio)\n",
    "hyperparameter_tuning_specific(df3_columns, 'DecisionTreeClassifier', arbolDecision)\n",
    "# hyperparameter_tuning_specific(df3_columns, 'NeuralNetwork', redNeuronal)\n",
    "\n",
    "\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(29.763514416313132)} con una puntuación de 0.8307701271904486\n",
    "# LogisticRegression 0.8307701271904486\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 200, 'loss': 'hinge', 'alpha': np.float64(3.5938136638046256e-05)} con una puntuación de 0.8355134628645823\n",
    "# SGDClassifier 0.8363256270804058\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(15), 'max_features': 'sqrt', 'max_depth': np.int64(5)} con una puntuación de 0.8407902015725431\n",
    "# RandomForestClassifier 0.8403833332635926\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'entropy'} con una puntuación de 0.8166143581770171\n",
    "# DecisionTreeClassifier 0.8166258721117126\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(29.763514416313132)} con una puntuación de 0.8307701271904486\n",
      "LogisticRegression 0.8307701271904486\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.001, 'max_iter': 200, 'loss': 'hinge', 'alpha': np.float64(3.5938136638046256e-05)} con una puntuación de 0.8336961748141649\n",
      "SGDClassifier 0.8157468825390846\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(15), 'max_features': 'sqrt', 'max_depth': np.int64(5)} con una puntuación de 0.840917093187246\n",
      "RandomForestClassifier 0.8404333706468845\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'entropy'} con una puntuación de 0.8165628703939516\n",
      "DecisionTreeClassifier 0.8165892306377781\n"
     ]
    }
   ],
   "source": [
    "df3_columns = read_pickle('df3_over_columns.pkl')\n",
    "\n",
    "hyperparameter_tuning_specific(df3_columns, 'LogisticRegression', regresionLogistica)\n",
    "hyperparameter_tuning_specific(df3_columns, 'SGDClassifier', SGDC)\n",
    "hyperparameter_tuning_specific(df3_columns, 'RandomForestClassifier', bosqueAleatorio)\n",
    "hyperparameter_tuning_specific(df3_columns, 'DecisionTreeClassifier', arbolDecision)\n",
    "# hyperparameter_tuning_specific(df3_columns, 'NeuralNetwork', redNeuronal)\n",
    "\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(29.763514416313132)} con una puntuación de 0.8307701271904486\n",
    "# LogisticRegression 0.8307701271904486\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.001, 'max_iter': 200, 'loss': 'hinge', 'alpha': np.float64(3.5938136638046256e-05)} con una puntuación de 0.8336961748141649\n",
    "# SGDClassifier 0.8157468825390846\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(15), 'max_features': 'sqrt', 'max_depth': np.int64(5)} con una puntuación de 0.840917093187246\n",
    "# RandomForestClassifier 0.8404333706468845\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'entropy'} con una puntuación de 0.8165628703939516\n",
    "# DecisionTreeClassifier 0.8165892306377781\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
