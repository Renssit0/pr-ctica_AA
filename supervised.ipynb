{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import F1Score\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow import multiply\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from itertools import product\n",
    "from tensorflow.random import set_seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos una búsqueda aleatorio de los mejores hiperparámetros, esto es debido a que una búsqueda más exhaustiva para un dataset tan grande es demasiado ineficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regresionLogistica(X, y, n, v_cruzada, seed):\n",
    "    log_reg = LogisticRegression(max_iter=500)\n",
    "    parametros_log_reg = {'C': np.logspace(-4, 4, 20), 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
    "    busqueda_log_reg = RandomizedSearchCV(estimator=log_reg, param_distributions=parametros_log_reg, n_iter=n, \\\n",
    "                                      scoring='f1_weighted', cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_log_reg.fit(X, y)\n",
    "    parametros = busqueda_log_reg.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_log_reg.best_score_}')\n",
    "\n",
    "    modelo = LogisticRegression(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def SGDC(X, y, n, v_cruzada, seed):\n",
    "    sgd = SGDClassifier()\n",
    "    parametros_sgdc = {'loss': ['hinge', 'squared_error', 'log_loss'], 'alpha': np.logspace(-5, 0, 10),\\\n",
    "                       'max_iter': [100, 200], 'tol': [1e-3, 1e-4]}\n",
    "    busqueda_sgdc = RandomizedSearchCV(estimator=sgd, param_distributions=parametros_sgdc, n_iter=n,\\\n",
    "                                       scoring='f1_weighted', cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_sgdc.fit(X, y)\n",
    "    parametros = busqueda_sgdc.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_sgdc.best_score_}')\n",
    "\n",
    "    modelo = SGDClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n",
    "def bosqueAleatorio(X, y, n, v_cruzada, seed):\n",
    "    rf = RandomForestClassifier()\n",
    "    parametros_rf = {'n_estimators': np.arange(100, 200, 20), 'max_features': ['sqrt', 'log2']+list(range(1, 10, 3)), \\\n",
    "                     'max_depth': list(np.arange(5, 20, 5)), 'min_samples_split': np.arange(5, 20, 5)}\n",
    "    busqueda_rf = RandomizedSearchCV(estimator=rf, param_distributions=parametros_rf, n_iter=n, scoring='f1_weighted',\\\n",
    "                                   cv = v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_rf.fit(X, y)\n",
    "    parametros = busqueda_rf.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_rf.best_score_}')\n",
    "\n",
    "    modelo = RandomForestClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def arbolDecision(X, y, n, v_cruzada, seed):\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    parametros_dtc = {'max_depth': list(np.arange(20, 50, 2)), 'min_samples_split': np.arange(10, 20, 2), 'criterion': ['gini', 'entropy']}\n",
    "    busqueda_dtc = RandomizedSearchCV(estimator=dtc, param_distributions=parametros_dtc, n_iter=n, scoring='f1_weighted',\\\n",
    "                                      cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_dtc.fit(X, y)\n",
    "    parametros = busqueda_dtc.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_dtc.best_score_}')\n",
    "\n",
    "    modelo = DecisionTreeClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n",
    "def redNeuronal(X, y, n, v_cruzada, seed):\n",
    "    set_seed(seed)\n",
    "    def design_model(n_features, lr):\n",
    "        input_ = Input(shape=(n_features,))\n",
    "        layer1 = Dropout(0.3)(input_)\n",
    "        layer2 = Dense(128, kernel_regularizer=L2)(layer1)\n",
    "        layer3 = Dropout(0.3)(layer2)\n",
    "        layer4 = Dense(128, kernel_regularizer=L2)(layer3)\n",
    "        output = Dense(1, activation=\"sigmoid\")(layer4)\n",
    "\n",
    "        # model\n",
    "        model = Model(inputs=input_, outputs=output)\n",
    "        model.compile(loss=BinaryCrossentropy, metrics=[F1Score], optimizer = Adam(learning_rate=lr))\n",
    "        return model\n",
    "    \n",
    "    n_features = X.shape[-1]\n",
    "    stop = EarlyStopping(monitor='val_loss', mode='min', patience=50, restore_best_weights=True)\n",
    "    parameter_grid = product((0.1, 0.01, 0.001), (16, 64, 256))\n",
    "\n",
    "    results = list()\n",
    "    for comb in parameter_grid:\n",
    "        model = design_model(n_features, lr=comb[0])\n",
    "        history = model.fit(X,y,epochs=500, batch_size=comb[1], validation_split=0.2,\n",
    "                        callbacks = [stop], verbose=0)\n",
    "        results.append((comb[0], comb[1], history.history['f1_score'][-1]))\n",
    "    \n",
    "    best_param = max(sorted(results, key = lambda x: x[2]))\n",
    "    print(f'Los mejores parámetros son: learning_rate: {best_param[0]}, num_batches: {best_param[1]} con una puntuación de {best_param[2]}')\n",
    "\n",
    "    model = design_model(n_features, lr=best_param[0])\n",
    "    scores = np.empty(v_cruzada)\n",
    "    n_samples = X.shape[0]\n",
    "    jump = int(n_samples/v_cruzada)\n",
    "    for i in range(v_cruzada):\n",
    "        val_idx = range(i*jump,(i+1)*jump)\n",
    "        model.fit(X.drop(X.iloc[val_idx].index), y.drop(y.iloc[val_idx].index),epochs=500, batch_size=comb[1], validation_data=(X.iloc[val_idx],y.iloc[val_idx]),\n",
    "                callbacks = [stop], verbose=0)\n",
    "        scores[i] = stop.best\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def KNN(X, y, n, v_cruzada, seed):\n",
    "    knn = KNeighborsClassifier()\n",
    "    parametros_knn = {'n_neighbors': np.arange(1, 15, 3), 'weights': ['uniform', 'distance'],\\\n",
    "                      'metric': ['euclidean', 'manhattan']}\n",
    "    busqueda_knn = RandomizedSearchCV(estimator=knn, param_distributions=parametros_knn, n_iter=n,\\\n",
    "                                    scoring='f1_weighted', cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_knn.fit(X, y)\n",
    "    parametros = busqueda_knn.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_knn.best_score_}')\n",
    "\n",
    "    modelo = KNeighborsClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if TensorFlow can detect the GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Available GPUs: {gpus}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_cruzada = 5\n",
    "seed = 34\n",
    "n = 30\n",
    "df = pd.read_csv('df.csv', index_col=0).sample(n=50000, random_state=seed)\n",
    "df_over = pd.read_csv('df_over.csv', index_col=0).sample(n=50000, random_state=seed)\n",
    "\n",
    "def read_pickle(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        columns = pickle.load(file)\n",
    "    return columns\n",
    "\n",
    "def hyperparameter_tuning_general(path):\n",
    "    df_rdx = df[read_pickle(path)]\n",
    "    X, y = df_rdx.drop('label', axis=1), df_rdx['label']\n",
    "    print(regresionLogistica(X, y, n, v_cruzada, seed))\n",
    "    print(SGDC(X, y, n, v_cruzada, seed))\n",
    "    print(bosqueAleatorio(X, y, n, v_cruzada, seed))\n",
    "    print(arbolDecision(X, y, n, v_cruzada, seed))\n",
    "    print(redNeuronal(X, y, n, v_cruzada, seed))\n",
    "    print(KNN(X, y, n, v_cruzada, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning_general(path):\n",
    "    df_rdx = df[read_pickle(path)]\n",
    "    X, y = df_rdx.drop('label', axis=1), df_rdx['label']\n",
    "    # print(regresionLogistica(X, y, n, v_cruzada, seed))\n",
    "    # print(SGDC(X, y, n, v_cruzada, seed))\n",
    "    # print(bosqueAleatorio(X, y, n, v_cruzada, seed))\n",
    "    # print(arbolDecision(X, y, n, v_cruzada, seed))\n",
    "    print(redNeuronal(X, y, n, v_cruzada, seed))\n",
    "    # print(KNN(X, y, n, v_cruzada, seed))\n",
    "              \n",
    "hyperparameter_tuning_general('df1_columns.pkl')\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(1438.44988828766)} con una puntuación de 0.8836523703898143\n",
    "# 0.8836523703898143\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(1e-05)} con una puntuación de 0.8830713771651768\n",
    "# 0.8705891820233645\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# c:\\Users\\\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
    "#   _data = np.array(data, dtype=dtype, copy=copy,\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(100), 'min_samples_split': np.int64(15), 'max_features': 'log2', 'max_depth': np.int64(15)} con una puntuación de 0.8822279091007633\n",
    "# 0.8802700901684221\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(18), 'max_depth': np.int64(22), 'criterion': 'gini'} con una puntuación de 0.8297647106742028\n",
    "# 0.8292204390533506\n",
    "# WARNING:tensorflow:From c:\\Users\\\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
    "\n",
    "# arreglar tuning, ventaja a quien converge mas rapido\n",
    "\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# c:\\Users\\\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
    "#   warnings.warn(\n",
    "# Los mejores parámetros son: {'weights': 'uniform', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8381402494954733\n",
    "# 0.8381402494954733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning_general(path):\n",
    "    df_rdx = df[read_pickle(path)]\n",
    "    X, y = df_rdx.drop('label', axis=1), df_rdx['label']\n",
    "    print(regresionLogistica(X, y, n, v_cruzada, seed))\n",
    "    print(SGDC(X, y, n, v_cruzada, seed))\n",
    "    print(bosqueAleatorio(X, y, n, v_cruzada, seed))\n",
    "    print(arbolDecision(X, y, n, v_cruzada, seed))\n",
    "    print(redNeuronal(X, y, n, v_cruzada, seed))\n",
    "    print(KNN(X, y, n, v_cruzada, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_tuning_general('df1_over_columns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_tuning_general('df21_columns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_tuning_general('df21_over_columns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_tuning_general('df22_columns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_tuning_general('df22_over_columns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = (regresionLogistica, SGDC, bosqueAleatorio, arbolDecision, red_neuronal)\n",
    "read_pickle('df3_columns.pkl') + read_pickle('df_NN_columns.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_pickle('df3_over_columns.pkl') + read_pickle('df_NN_over_columns.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
