{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import F1Score\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow import multiply\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from itertools import product\n",
    "from tensorflow.random import set_seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para búsqueda de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regresionLogistica(X, y, n, v_cruzada, seed):\n",
    "    log_reg = LogisticRegression(max_iter=500)\n",
    "    parametros_log_reg = {'C': np.logspace(-4, 4, 20), 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
    "    busqueda_log_reg = RandomizedSearchCV(estimator=log_reg, param_distributions=parametros_log_reg, n_iter=n, \\\n",
    "                                      scoring='f1_weighted', cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_log_reg.fit(X, y)\n",
    "    parametros = busqueda_log_reg.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_log_reg.best_score_}')\n",
    "\n",
    "    modelo = LogisticRegression(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return (parametros, scores.mean())\n",
    "\n",
    "\n",
    "def SGDC(X, y, n, v_cruzada, seed):\n",
    "    sgd = SGDClassifier()\n",
    "    parametros_sgdc = {'loss': ['hinge', 'squared_error', 'log_loss'], 'alpha': np.logspace(-5, 0, 10),\\\n",
    "                       'max_iter': [100, 200], 'tol': [1e-3, 1e-4]}\n",
    "    busqueda_sgdc = RandomizedSearchCV(estimator=sgd, param_distributions=parametros_sgdc, n_iter=n,\\\n",
    "                                       scoring='f1_weighted', cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_sgdc.fit(X, y)\n",
    "    parametros = busqueda_sgdc.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_sgdc.best_score_}')\n",
    "\n",
    "    modelo = SGDClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return (parametros, scores.mean())\n",
    "\n",
    "def bosqueAleatorio(X, y, n, v_cruzada, seed):\n",
    "    rf = RandomForestClassifier()\n",
    "    parametros_rf = {'n_estimators': np.arange(100, 200, 20), 'max_features': ['sqrt', 'log2']+list(range(1, 10, 3)), \\\n",
    "                     'max_depth': list(np.arange(5, 20, 5)), 'min_samples_split': np.arange(5, 20, 5)}\n",
    "    busqueda_rf = RandomizedSearchCV(estimator=rf, param_distributions=parametros_rf, n_iter=n, scoring='f1_weighted',\\\n",
    "                                   cv = v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_rf.fit(X, y)\n",
    "    parametros = busqueda_rf.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_rf.best_score_}')\n",
    "\n",
    "    modelo = RandomForestClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return (parametros, scores.mean())\n",
    "\n",
    "def arbolDecision(X, y, n, v_cruzada, seed):\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    parametros_dtc = {'max_depth': list(np.arange(20, 50, 2)), 'min_samples_split': np.arange(10, 20, 2), 'criterion': ['gini', 'entropy']}\n",
    "    busqueda_dtc = RandomizedSearchCV(estimator=dtc, param_distributions=parametros_dtc, n_iter=n, scoring='f1_weighted',\\\n",
    "                                      cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_dtc.fit(X, y)\n",
    "    parametros = busqueda_dtc.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_dtc.best_score_}')\n",
    "\n",
    "    modelo = DecisionTreeClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return (parametros, scores.mean())\n",
    "\n",
    "def redNeuronal(X, y, n, v_cruzada, seed):\n",
    "    set_seed(seed)\n",
    "    def design_model(n_features, lr):\n",
    "        input_ = Input(shape=(n_features,))\n",
    "        layer1 = Dropout(0.3)(input_)\n",
    "        layer2 = Dense(128, kernel_regularizer=L2)(layer1)\n",
    "        layer3 = Dropout(0.3)(layer2)\n",
    "        layer4 = Dense(128, kernel_regularizer=L2)(layer3)\n",
    "        output = Dense(1, activation=\"sigmoid\")(layer4)\n",
    "\n",
    "        # model\n",
    "        model = Model(inputs=input_, outputs=output)\n",
    "        model.compile(loss=BinaryCrossentropy, metrics=[F1Score], optimizer = Adam(learning_rate=lr))\n",
    "        return model\n",
    "    \n",
    "    n_features = X.shape[-1]\n",
    "    stop = EarlyStopping(monitor='val_loss', mode='min', patience=50, restore_best_weights=True)\n",
    "    parameter_grid = product((0.1, 0.01, 0.001), (16, 64, 256))\n",
    "\n",
    "    results = list()\n",
    "    for comb in parameter_grid:\n",
    "        model = design_model(n_features, lr=comb[0])\n",
    "        history = model.fit(X,y,epochs=500, batch_size=comb[1], validation_split=0.2,\n",
    "                        callbacks = [stop], verbose=0)\n",
    "        results.append((comb[0], comb[1], history.history['f1_score'][-1]))\n",
    "    \n",
    "    best_param = max(sorted(results, key = lambda x: x[2]))\n",
    "    print(f'Los mejores parámetros son: learning_rate: {best_param[0]}, num_batches: {best_param[1]} con una puntuación de {best_param[2]}')\n",
    "\n",
    "    model = design_model(n_features, lr=best_param[0])\n",
    "    scores = np.empty(v_cruzada)\n",
    "    n_samples = X.shape[0]\n",
    "    jump = int(n_samples/v_cruzada)\n",
    "    for i in range(v_cruzada):\n",
    "        val_idx = range(i*jump,(i+1)*jump)\n",
    "        model.fit(X.drop(X.iloc[val_idx].index), y.drop(y.iloc[val_idx].index),epochs=500, batch_size=comb[1], validation_data=(X.iloc[val_idx],y.iloc[val_idx]),\n",
    "                callbacks = [stop], verbose=0)\n",
    "        scores[i] = stop.best\n",
    "\n",
    "    return (best_param, scores.mean())\n",
    "\n",
    "\n",
    "def KNN(X, y, n, v_cruzada, seed):\n",
    "    knn = KNeighborsClassifier()\n",
    "    parametros_knn = {'n_neighbors': np.arange(1, 15, 3), 'weights': ['uniform', 'distance'],\\\n",
    "                      'metric': ['euclidean', 'manhattan']}\n",
    "    busqueda_knn = RandomizedSearchCV(estimator=knn, param_distributions=parametros_knn, n_iter=n,\\\n",
    "                                    scoring='f1_weighted', cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_knn.fit(X, y)\n",
    "    parametros = busqueda_knn.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_knn.best_score_}')\n",
    "\n",
    "    modelo = KNeighborsClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return (parametros, scores.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_cruzada = 5\n",
    "seed = 34\n",
    "n = 30\n",
    "df = pd.read_csv('df.csv', index_col=0).sample(n=50000, random_state=seed)\n",
    "df_over = pd.read_csv('df_over.csv', index_col=0).sample(n=50000, random_state=seed)\n",
    "df = pd.read_csv('df.csv', index_col=0).sample(n=50000, random_state=seed)\n",
    "df = pd.read_csv('df.csv', index_col=0).sample(n=50000, random_state=seed)\n",
    "\n",
    "def read_pickle(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        columns = pickle.load(file)\n",
    "    return columns\n",
    "\n",
    "def hyperparameter_tuning_general(path):\n",
    "    df_rdx = df[read_pickle(path)]\n",
    "    X, y = df_rdx.drop('label', axis=1), df_rdx['label']\n",
    "\n",
    "    return {'LogisticRegression': regresionLogistica(X, y, n, v_cruzada, seed), \n",
    "            'SGDClassifier':SGDC(X, y, n, v_cruzada, seed), \n",
    "            'RandomForestClassifier':bosqueAleatorio(X, y, n, v_cruzada, seed), \n",
    "            'DecisionTreeClassifier':arbolDecision(X, y, n, v_cruzada, seed), \n",
    "            'KNeighborsClassifier':KNN(X, y, n, v_cruzada, seed),\n",
    "            # 'NeuralNetwork': redNeuronal(X, y, n, v_cruzada, seed)\n",
    "            }\n",
    "\n",
    "def hyperparameter_tuning_NN(path):\n",
    "    df_rdx = df[read_pickle(path)]\n",
    "    X, y = df_rdx.drop('label', axis=1), df_rdx['label']\n",
    "    return {'NeuralNetwork': redNeuronal(X, y, n, v_cruzada, seed)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda simultánea: mejores hiperparámetros, mejores modelos, mejor balanceo de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "scores.append(hyperparameter_tuning_general('df21_columns.pkl'))\n",
    "scores.append(hyperparameter_tuning_general('df22_columns.pkl'))\n",
    "scores.append(hyperparameter_tuning_general('df21_over_columns.pkl'))\n",
    "scores.append(hyperparameter_tuning_general('df22_over_columns.pkl'))\n",
    "scores.append(hyperparameter_tuning_general('df21_under_columns.pkl'))\n",
    "scores.append(hyperparameter_tuning_general('df22_under_columns.pkl'))\n",
    "scores.append(hyperparameter_tuning_general('df21_mid_columns.pkl'))\n",
    "scores.append(hyperparameter_tuning_general('df22_mid_columns.pkl'))\n",
    "with open('scores.pkl', 'wb') as file:\n",
    "    pickle.dump((scores), file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8523182234405609\n",
      "0.8600814040560797 \n",
      "\n",
      "0.8545899396164872\n",
      "\n",
      "0.8563603119490267\n",
      "\n",
      "0.8562978987983811\n",
      "\n",
      "0.8575511046293863\n",
      "\n",
      "[[0.8685737  0.86112893 0.87033156 0.82262689 0.83599927]\n",
      " [0.88173491 0.85709983 0.88045592 0.83164272 0.83630565]\n",
      " [0.86906003 0.86420349 0.8699391  0.82237719 0.83676863]\n",
      " [0.88198528 0.86930574 0.88068736 0.83183509 0.83744121]\n",
      " [0.86675366 0.86469121 0.86915533 0.82177005 0.83611171]\n",
      " [0.88192675 0.87298375 0.88100598 0.83118532 0.83739522]\n",
      " [0.86906003 0.86900508 0.87000993 0.82203004 0.83676863]\n",
      " [0.88205688 0.87587536 0.88082719 0.83163001 0.83824791]]\n"
     ]
    }
   ],
   "source": [
    "# mejores hiperparámetros, mejores modelos, mejores balanceos\n",
    "with open('scores.pkl', 'rb') as file:\n",
    "    scores = pickle.load(file)\n",
    "\n",
    "models = ['LogisticRegression', 'SGDClassifier', 'RandomForestClassifier', 'DecisionTreeClassifier', 'KNeighborsClassifier']\n",
    "param = 0\n",
    "score = 1\n",
    "\n",
    "best_params = dict()\n",
    "model_scores = dict()\n",
    "score_matrix = np.empty((8, len(models)))\n",
    "\n",
    "i = 0\n",
    "for model in models:\n",
    "    hiperparameter_dataframe = pd.DataFrame([scores[i][model][0] for i in range(len(scores))])\n",
    "    score_dataframe = pd.DataFrame([scores[i][model][1] for i in range(len(scores))])\n",
    "    complete_dataframe = pd.concat([hiperparameter_dataframe, score_dataframe], axis=1)\n",
    "\n",
    "    best_param = hiperparameter_dataframe.mode().iloc[0].to_dict()\n",
    "    best_params[model] = best_param\n",
    "\n",
    "    best_score = score_dataframe.max().iloc[0]\n",
    "    avg_score = score_dataframe.mean().iloc[0]\n",
    "    model_scores[model] = {'best':best_score, 'average':avg_score}\n",
    "\n",
    "    score_matrix[:,i] = score_dataframe.values.reshape(1,-1)\n",
    "    i+= 1\n",
    "\n",
    "    # print(model)\n",
    "    # display(complete_dataframe)\n",
    "    # display(hiperparameter_dataframe.mode())\n",
    "    # print(best_param)\n",
    "    # print()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# hiperparametro claro excepto para SGDC\n",
    "\n",
    "# -------------------------------------------------\n",
    "# model scores visualization: small margin 1.Logistic Regression, 2. RandomForest, 3. SGDClassifier\n",
    "# pd.DataFrame(model_scores).T.plot.bar()\n",
    "# plt.ylim((0.8,0.9))\n",
    "# plt.show()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# best balance and filtering (filtering juega un papel, pero no es el objetivo principal)\n",
    "\n",
    "    # diferencia entre método de filtrado\n",
    "print(score_matrix[::2].mean()) # variance threshold\n",
    "print(score_matrix[1::2].mean(), '\\n') # k best\n",
    "    # hay mayor una diferencia del >0.01 entre ambos métodos de filtrado usado\n",
    "\n",
    "# diferencia entre método de balanceado\n",
    "for i in range(4): # mse\n",
    "    print(score_matrix[2*i:2*(i+1)].mean())\n",
    "    print()\n",
    "print(score_matrix)\n",
    "# hay una diferencia >0.01, con el mixto siendo el mejor, aunque la diferencia no sea significativa para escoger un método de balanceado sobresaliente, \n",
    "# tampoco importaría en el resultado final si se escoge preliminalmente uno que otro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chaos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: learning_rate: 0.1, num_batches: 256 con una puntuación de 0.4108065068721771\n",
      "0.3101118505001068\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning_general('df1_columns.pkl')\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(1438.44988828766)} con una puntuación de 0.8836523703898143\n",
    "# 0.8836523703898143\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(1e-05)} con una puntuación de 0.8830713771651768\n",
    "# 0.8705891820233645\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(100), 'min_samples_split': np.int64(15), 'max_features': 'log2', 'max_depth': np.int64(15)} con una puntuación de 0.8822279091007633\n",
    "# 0.8802700901684221\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(18), 'max_depth': np.int64(22), 'criterion': 'gini'} con una puntuación de 0.8297647106742028\n",
    "# 0.8292204390533506\n",
    "\n",
    "# arreglar tuning, ventaja a quien converge mas rapido\n",
    "# Los mejores parámetros son: learning_rate: 0.1, num_batches: 256 con una puntuación de 0.4108065068721771\n",
    "# 0.3101118505001068\n",
    "\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# Los mejores parámetros son: {'weights': 'uniform', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8381402494954733\n",
    "# 0.8381402494954733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': np.float64(11.288378916846883)} con una puntuación de 0.8790116083084459\n",
      "0.8789239112218639\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.001, 'max_iter': 200, 'loss': 'log_loss', 'alpha': np.float64(1e-05)} con una puntuación de 0.8763173410753795\n",
      "0.870363909928772\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(10), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.8798856772816105\n",
      "0.8797234595037231\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8281086733314857\n",
      "0.8277098130939763\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8376863481234462\n",
      "0.8376863481234462\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning_general('df1_over_columns.pkl')\n",
    "\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': np.float64(11.288378916846883)} con una puntuación de 0.8790116083084459\n",
    "# 0.8789239112218639\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.001, 'max_iter': 200, 'loss': 'log_loss', 'alpha': np.float64(1e-05)} con una puntuación de 0.8763173410753795\n",
    "# 0.870363909928772\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(10), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.8798856772816105\n",
    "# 0.8797234595037231\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8281086733314857\n",
    "# 0.8277098130939763\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8376863481234462\n",
    "# 0.8376863481234462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': np.float64(1.623776739188721)} con una puntuación de 0.8686068480450551\n",
      "0.8685884377243067\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(0.0001291549665014884)} con una puntuación de 0.869123169398431\n",
      "0.8680290383424178\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(5), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.870872257158875\n",
      "0.8693060540725103\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8238308603518425\n",
      "0.8229223637389049\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: {'weights': 'uniform', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8359992668647557\n",
      "0.8359992668647557\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning_general('df21_columns.pkl')\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': np.float64(1.623776739188721)} con una puntuación de 0.8686068480450551\n",
    "# 0.8685884377243067\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(0.0001291549665014884)} con una puntuación de 0.869123169398431\n",
    "# 0.8680290383424178\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(5), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.870872257158875\n",
    "# 0.8693060540725103\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8238308603518425\n",
    "# 0.8229223637389049\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# Los mejores parámetros son: {'weights': 'uniform', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8359992668647557\n",
    "# 0.8359992668647557"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': np.float64(3792.690190732246)} con una puntuación de 0.8690637259804713\n",
      "0.8689902079249718\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.001, 'max_iter': 200, 'loss': 'hinge', 'alpha': np.float64(0.0016681005372000592)} con una puntuación de 0.8695568774417272\n",
      "0.8666602968564856\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(10), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.8706375912843614\n",
      "0.8699943036851534\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8232128288147627\n",
      "0.8231848480889429\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8367686275629154\n",
      "0.8367686275629154\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning_general('df21_over_columns.pkl')\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': np.float64(3792.690190732246)} con una puntuación de 0.8690637259804713\n",
    "# 0.8689902079249718\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.001, 'max_iter': 200, 'loss': 'hinge', 'alpha': np.float64(0.0016681005372000592)} con una puntuación de 0.8695568774417272\n",
    "# 0.8666602968564856\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(10), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.8706375912843614\n",
    "# 0.8699943036851534\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8232128288147627\n",
    "# 0.8231848480889429\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8367686275629154\n",
    "# 0.8367686275629154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(10000.0)} con una puntuación de 0.8817349147911596\n",
      "0.8817349147911596\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(0.0001291549665014884)} con una puntuación de 0.8797487306986795\n",
      "0.877929648312875\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(10), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.8811952711092832\n",
      "0.8803406313736026\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8308660422307245\n",
      "0.8308985042919395\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8363056535870994\n",
      "0.8363056535870994\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning_general('df22_columns.pkl')\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(10000.0)} con una puntuación de 0.8817349147911596\n",
    "# 0.8817349147911596\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(0.0001291549665014884)} con una puntuación de 0.8797487306986795\n",
    "# 0.877929648312875\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(10), 'max_features': 'sqrt', 'max_depth': np.int64(15)} con una puntuación de 0.8811952711092832\n",
    "# 0.8803406313736026\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8308660422307245\n",
    "# 0.8308985042919395\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8363056535870994\n",
    "# 0.8363056535870994\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(3792.690190732246)} con una puntuación de 0.8819852792810661\n",
      "0.8819852792810661\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(0.0001291549665014884)} con una puntuación de 0.8802596768085641\n",
      "0.8802367991149194\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(5), 'max_features': 'log2', 'max_depth': np.int64(15)} con una puntuación de 0.8807910837927398\n",
      "0.8799058676583137\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8313389385364051\n",
      "0.8320685712582645\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8374412069902707\n",
      "0.8374412069902707\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning_general('df22_over_columns.pkl')\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(3792.690190732246)} con una puntuación de 0.8819852792810661\n",
    "# 0.8819852792810661\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 100, 'loss': 'hinge', 'alpha': np.float64(0.0001291549665014884)} con una puntuación de 0.8802596768085641\n",
    "# 0.8802367991149194\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(5), 'max_features': 'log2', 'max_depth': np.int64(15)} con una puntuación de 0.8807910837927398\n",
    "# 0.8799058676583137\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'gini'} con una puntuación de 0.8313389385364051\n",
    "# 0.8320685712582645\n",
    "# Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "# c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
    "#   warnings.warn(\n",
    "# Los mejores parámetros son: {'weights': 'distance', 'n_neighbors': np.int64(13), 'metric': 'manhattan'} con una puntuación de 0.8374412069902707\n",
    "# 0.8374412069902707\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning_specific(df_columns, name, model):\n",
    "    df_rdx = df[df_columns[name]]\n",
    "    X, y = df_rdx.drop('label', axis=1), df_rdx['label'] \n",
    "    print(name, model(X, y, n, v_cruzada, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(29.763514416313132)} con una puntuación de 0.8307701271904486\n",
      "LogisticRegression 0.8307701271904486\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 200, 'loss': 'hinge', 'alpha': np.float64(3.5938136638046256e-05)} con una puntuación de 0.8355134628645823\n",
      "SGDClassifier 0.8363256270804058\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(15), 'max_features': 'sqrt', 'max_depth': np.int64(5)} con una puntuación de 0.8407902015725431\n",
      "RandomForestClassifier 0.8403833332635926\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'entropy'} con una puntuación de 0.8166143581770171\n",
      "DecisionTreeClassifier 0.8166258721117126\n"
     ]
    }
   ],
   "source": [
    "df3_columns = read_pickle('df3_columns.pkl')\n",
    "\n",
    "hyperparameter_tuning_specific(df3_columns, 'LogisticRegression', regresionLogistica)\n",
    "hyperparameter_tuning_specific(df3_columns, 'SGDClassifier', SGDC)\n",
    "hyperparameter_tuning_specific(df3_columns, 'RandomForestClassifier', bosqueAleatorio)\n",
    "hyperparameter_tuning_specific(df3_columns, 'DecisionTreeClassifier', arbolDecision)\n",
    "# hyperparameter_tuning_specific(df3_columns, 'NeuralNetwork', redNeuronal)\n",
    "\n",
    "\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(29.763514416313132)} con una puntuación de 0.8307701271904486\n",
    "# LogisticRegression 0.8307701271904486\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.0001, 'max_iter': 200, 'loss': 'hinge', 'alpha': np.float64(3.5938136638046256e-05)} con una puntuación de 0.8355134628645823\n",
    "# SGDClassifier 0.8363256270804058\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(15), 'max_features': 'sqrt', 'max_depth': np.int64(5)} con una puntuación de 0.8407902015725431\n",
    "# RandomForestClassifier 0.8403833332635926\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'entropy'} con una puntuación de 0.8166143581770171\n",
    "# DecisionTreeClassifier 0.8166258721117126\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(29.763514416313132)} con una puntuación de 0.8307701271904486\n",
      "LogisticRegression 0.8307701271904486\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'tol': 0.001, 'max_iter': 200, 'loss': 'hinge', 'alpha': np.float64(3.5938136638046256e-05)} con una puntuación de 0.8336961748141649\n",
      "SGDClassifier 0.8157468825390846\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(15), 'max_features': 'sqrt', 'max_depth': np.int64(5)} con una puntuación de 0.840917093187246\n",
      "RandomForestClassifier 0.8404333706468845\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'entropy'} con una puntuación de 0.8165628703939516\n",
      "DecisionTreeClassifier 0.8165892306377781\n"
     ]
    }
   ],
   "source": [
    "df3_columns = read_pickle('df3_over_columns.pkl')\n",
    "\n",
    "hyperparameter_tuning_specific(df3_columns, 'LogisticRegression', regresionLogistica)\n",
    "hyperparameter_tuning_specific(df3_columns, 'SGDClassifier', SGDC)\n",
    "hyperparameter_tuning_specific(df3_columns, 'RandomForestClassifier', bosqueAleatorio)\n",
    "hyperparameter_tuning_specific(df3_columns, 'DecisionTreeClassifier', arbolDecision)\n",
    "# hyperparameter_tuning_specific(df3_columns, 'NeuralNetwork', redNeuronal)\n",
    "\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(29.763514416313132)} con una puntuación de 0.8307701271904486\n",
    "# LogisticRegression 0.8307701271904486\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'tol': 0.001, 'max_iter': 200, 'loss': 'hinge', 'alpha': np.float64(3.5938136638046256e-05)} con una puntuación de 0.8336961748141649\n",
    "# SGDClassifier 0.8157468825390846\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'n_estimators': np.int64(180), 'min_samples_split': np.int64(15), 'max_features': 'sqrt', 'max_depth': np.int64(5)} con una puntuación de 0.840917093187246\n",
    "# RandomForestClassifier 0.8404333706468845\n",
    "# Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "# Los mejores parámetros son: {'min_samples_split': np.int64(16), 'max_depth': np.int64(20), 'criterion': 'entropy'} con una puntuación de 0.8165628703939516\n",
    "# DecisionTreeClassifier 0.8165892306377781\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
